# Backend
DATABASE_URL=sqlite:///./dev.db
# Postgres example:
# DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/narratives
# Ingest client: use 127.0.0.1 so it reaches the backend (localhost can resolve to IPv6 and cause 503)
API_BASE_URL=http://127.0.0.1:8000

# Storage
STORAGE_BACKEND=local  # gcs|local
GCS_BUCKET=your-bucket-name
GCS_PREFIX=investing-agent
LOCAL_STORAGE_DIR=.local_storage

# Embeddings: no Vertex required. Use OpenAI (same key as LLM) or Vertex.
# EMBEDDING_PROVIDER=auto   # auto | openai | vertex | none. auto = openai when Vertex off and LLM is openai
# EMBEDDING_MODEL=text-embedding-3-small   # for OpenAI (Vertex uses VERTEX_EMBED_MODEL)

# Vertex AI (optional; for embeddings + Gemini on GCP; leave ENABLE_VERTEX=false to use OpenAI embeddings only)
GCP_PROJECT=your-gcp-project
GCP_LOCATION=us-central1
VERTEX_GEMINI_MODEL=gemini-2.0-flash
VERTEX_EMBED_MODEL=gemini-embedding-001
ENABLE_VERTEX=false

# Simple LLM API (MVP: use API key for theme extraction, no Vertex required)
# Set LLM_API_KEY to use real LLM extraction; otherwise heuristic fallback is used.
LLM_PROVIDER=openai
LLM_API_KEY=
LLM_MODEL=gpt-4o-mini
# Optional base URL for OpenAI-compatible APIs (e.g. DeepSeek, Qwen):
# LLM_BASE_URL=https://api.deepseek.com
# Request timeout in seconds for LLM calls (Gemini can be slow on long documents; default 180).
# LLM_TIMEOUT_SECONDS=300
# Optional delay in seconds after each LLM extraction (e.g. 1.0 to stay under OpenAI RPM with 1000+ docs).
# LLM_DELAY_AFTER_REQUEST_SECONDS=1.0
# Force heuristic extraction only (no LLM/Vertex). When true, ignores LLM_API_KEY and Vertex.
USE_HEURISTIC_EXTRACTION=false

# Theme deduplication: similarity-based resolution (when exact/alias match fails).
# THEME_SIMILARITY_USE_EMBEDDING=true   # use Vertex embedding similarity when Vertex is enabled (default true)
# THEME_SIMILARITY_USE_TEXT=true        # use token (Dice) text similarity; no API required (default true)
# THEME_SIMILARITY_EMBEDDING_THRESHOLD=0.92   # min cosine similarity to merge, 0–1 (default 0.92)
# THEME_SIMILARITY_TEXT_THRESHOLD=0.7   # min Dice coefficient for token similarity, 0–1 (default 0.7)

# Theme merge suggestion (admin suggest-merges / run_theme_merges.py): label + content-aware embedding.
# THEME_MERGE_SUGGESTION_EMBEDDING_THRESHOLD=0.92   # label embedding similarity (default 0.92, higher = stricter)
# THEME_MERGE_USE_LLM_SUGGEST=false    # when true, also use LLM to suggest merge groups
# THEME_MERGE_USE_CONTENT_EMBEDDING=true   # use narratives + quotes for embedding (default true)
# THEME_MERGE_CONTENT_EMBEDDING_THRESHOLD=0.90   # content embedding similarity (default 0.90, higher = stricter)
# THEME_MERGE_REQUIRE_BOTH_EMBEDDINGS=true   # when true, only merge if BOTH label and content sim pass (default true)
# THEME_MERGE_MAX_NARRATIVES_PER_THEME=5
# THEME_MERGE_MAX_QUOTES_PER_THEME=8
# THEME_MERGE_MAX_QUOTE_CHARS=250

# Market data: Alpha Vantage for instrument price chart and P/E.
# ALPHA_VANTAGE_API_KEY=your_alpha_vantage_key
# Cache TTL in seconds (default 7200 = 2h). Increase to reduce API calls.
# ALPHA_VANTAGE_CACHE_TTL_SECONDS=7200
# Min seconds between API requests (default 1.0; 75/min limit ≈ 0.8s, 1.0 leaves headroom).
# ALPHA_VANTAGE_MIN_SECONDS_BETWEEN_REQUESTS=1.0

# Optional: write backend logs to this file (worker + API). Leave empty for stdout only.
# LOG_FILE=backend/logs/backend.log

# Ingest queue cap: reject new /ingest and /ingest-file when queued+processing jobs >= this (0 = no cap).
# Set to e.g. 100 to avoid runaway OpenAI usage when many documents are uploaded at once.
# MAX_QUEUED_INGEST_JOBS=100

# Temporary pause: when true, no new files are accepted (use while developing on existing data).
# PAUSE_INGEST=true

# Gmail daily sync: when true, run scripts/gmail_to_ingest.py in a background thread when the API server starts.
# First run after GMAIL_DAILY_SYNC_INITIAL_DELAY_SECONDS, then every GMAIL_DAILY_SYNC_INTERVAL_SECONDS.
# ENABLE_GMAIL_DAILY_SYNC=true
# GMAIL_DAILY_SYNC_INTERVAL_SECONDS=86400
# GMAIL_DAILY_SYNC_INITIAL_DELAY_SECONDS=60
# GMAIL_SYNC_PYTHON=python3   # default; set to a path (e.g. backend/.venv/bin/python) to use that interpreter for the script

# Optional: VPN/proxy for Gemini/OpenAI (see LLM_SETUP.md). Backend loads .env into process at startup.
# HTTPS_PROXY=http://proxy.example.com:8080
# HTTP_PROXY=http://proxy.example.com:8080

# Auth (MVP: off)
ENABLE_AUTH=false

# Ingest client (optional: if unset, uses repo root watch_pdfs/ and creates it)
# WATCH_DIR=/path/to/your/folder/with/pdfs
POLL_SECONDS=5

